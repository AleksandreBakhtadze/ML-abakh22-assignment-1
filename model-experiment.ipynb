{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:31.152271Z","iopub.execute_input":"2025-04-10T14:33:31.152639Z","iopub.status.idle":"2025-04-10T14:33:36.178244Z","shell.execute_reply.started":"2025-04-10T14:33:31.152614Z","shell.execute_reply":"2025-04-10T14:33:36.176928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:39.582338Z","iopub.execute_input":"2025-04-10T14:33:39.583438Z","iopub.status.idle":"2025-04-10T14:33:39.592469Z","shell.execute_reply.started":"2025-04-10T14:33:39.583401Z","shell.execute_reply":"2025-04-10T14:33:39.591205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# To see all columns","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)  \npd.set_option('display.width', None)        \npd.set_option('display.expand_frame_repr', False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:40.572843Z","iopub.execute_input":"2025-04-10T14:33:40.573145Z","iopub.status.idle":"2025-04-10T14:33:40.577953Z","shell.execute_reply.started":"2025-04-10T14:33:40.573123Z","shell.execute_reply":"2025-04-10T14:33:40.577006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read and Split data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:41.777763Z","iopub.execute_input":"2025-04-10T14:33:41.778059Z","iopub.status.idle":"2025-04-10T14:33:41.811246Z","shell.execute_reply.started":"2025-04-10T14:33:41.778038Z","shell.execute_reply":"2025-04-10T14:33:41.810384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:42.485828Z","iopub.execute_input":"2025-04-10T14:33:42.486193Z","iopub.status.idle":"2025-04-10T14:33:42.501690Z","shell.execute_reply.started":"2025-04-10T14:33:42.486171Z","shell.execute_reply":"2025-04-10T14:33:42.500600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Id columns","metadata":{}},{"cell_type":"code","source":"train_ids = X_train.pop('Id')\ntest_ids = X_test.pop('Id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:43.467730Z","iopub.execute_input":"2025-04-10T14:33:43.468034Z","iopub.status.idle":"2025-04-10T14:33:43.475154Z","shell.execute_reply.started":"2025-04-10T14:33:43.468012Z","shell.execute_reply":"2025-04-10T14:33:43.473739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build processor to handle nulls in data","metadata":{}},{"cell_type":"code","source":"def custom_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    # 1. Drop columns with 80% or more missing values\n    threshold = 0.8\n    null_fraction = df.isnull().mean()\n    cols_to_drop = null_fraction[null_fraction >= threshold].index\n    df = df.drop(columns=cols_to_drop)\n\n    # 2. Separate numerical and categorical columns\n    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    categoric_cols = df.select_dtypes(include=['object']).columns\n\n    # 3. Fill numeric NaNs with 0\n    df[numeric_cols] = df[numeric_cols].fillna(0)\n\n    # 4. Fill categoric NaNs with \"NO\"\n    df[categoric_cols] = df[categoric_cols].fillna(\"NO\")\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:44.308125Z","iopub.execute_input":"2025-04-10T14:33:44.308645Z","iopub.status.idle":"2025-04-10T14:33:44.315150Z","shell.execute_reply.started":"2025-04-10T14:33:44.308614Z","shell.execute_reply":"2025-04-10T14:33:44.314173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_cleaned = custom_preprocess(X_train.copy())\nX_test_cleaned = custom_preprocess(X_test.copy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:44.521774Z","iopub.execute_input":"2025-04-10T14:33:44.522075Z","iopub.status.idle":"2025-04-10T14:33:44.575354Z","shell.execute_reply.started":"2025-04-10T14:33:44.522054Z","shell.execute_reply":"2025-04-10T14:33:44.574527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split data to process with WOE and OHE","metadata":{}},{"cell_type":"code","source":"cat_cols = [col for col in X_train_cleaned.columns if X_train_cleaned[col].dtype == 'object']\nnum_cols = [col for col in X_train_cleaned.columns if X_train_cleaned[col].dtype != 'object']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:45.303973Z","iopub.execute_input":"2025-04-10T14:33:45.304732Z","iopub.status.idle":"2025-04-10T14:33:45.313440Z","shell.execute_reply.started":"2025-04-10T14:33:45.304705Z","shell.execute_reply":"2025-04-10T14:33:45.312418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s = X_train_cleaned[cat_cols].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:45.888626Z","iopub.execute_input":"2025-04-10T14:33:45.888961Z","iopub.status.idle":"2025-04-10T14:33:45.906266Z","shell.execute_reply.started":"2025-04-10T14:33:45.888937Z","shell.execute_reply":"2025-04-10T14:33:45.905381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 3\n\nwoe_columns = list(s[s > 3].index)\none_hot_columns = list(s[s <= 3].index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:46.055975Z","iopub.execute_input":"2025-04-10T14:33:46.056432Z","iopub.status.idle":"2025-04-10T14:33:46.062893Z","shell.execute_reply.started":"2025-04-10T14:33:46.056400Z","shell.execute_reply":"2025-04-10T14:33:46.061586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_cleaned[woe_columns].mode().T[0].to_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:46.635456Z","iopub.execute_input":"2025-04-10T14:33:46.635808Z","iopub.status.idle":"2025-04-10T14:33:46.656802Z","shell.execute_reply.started":"2025-04-10T14:33:46.635787Z","shell.execute_reply":"2025-04-10T14:33:46.655456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Preprocessor class","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CustomPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, woe_columns, one_hot_columns):\n        self.woe_columns = woe_columns # Columns Which Should Be Preprocessed Using WOE\n        self.one_hot_columns = one_hot_columns # Columns Which Should Be Preprocessed Using One Hot Encoder\n\n    def fit(self, X, y):\n        # Generate Fill Na Values Just in Case\n        self.woe_columns_fill_na = X[woe_columns].mode().T[0].to_dict()\n        \n        df_woe = X.copy()\n        target_col = 'target'\n        df_woe[target_col] = y\n\n        woe_mappings = {}\n        iv_values = {}\n        \n        for col in self.woe_columns:\n            print(f\"Processing {col}...\")\n            \n            groups = df_woe.groupby([col])[target_col].agg(['count', 'sum'])\n            groups.columns = ['n_obs', 'n_pos']\n            groups['n_neg'] = groups['n_obs'] - groups['n_pos']\n            \n            groups['prop_pos'] = groups['n_pos'] / groups['n_pos'].sum()\n            groups['prop_neg'] = groups['n_neg'] / groups['n_neg'].sum()\n            \n            groups['woe'] = np.log(groups['prop_pos'] / groups['prop_neg'])\n            groups['iv'] = (groups['prop_pos'] - groups['prop_neg']) * groups['woe']\n            \n            groups.replace([np.inf, -np.inf], 0, inplace=True)\n            groups.fillna(0, inplace=True)\n            \n            woe_dict = groups['woe'].to_dict()\n            iv = groups['iv'].sum()\n            \n            woe_mappings[col] = woe_dict\n            iv_values[col] = iv\n\n        self.woe_mappings = woe_mappings\n        self.iv_values = iv_values\n\n        return self\n\n\n    def transform(self, X):\n        X_transformed = X.copy()\n\n        # Preprocess WOE Columns\n        print(\"***\")\n        print(\"Preprocessing WOE Columns\")\n        for col in self.woe_columns:\n            X_transformed[f'{col}_woe'] = X_transformed[col].map(self.woe_mappings[col])\n            X_transformed.drop(columns=col, inplace=True)\n\n        print(\"Preprocessing One Hot Columns\")\n        X_transformed = pd.get_dummies(X_transformed, columns=self.one_hot_columns, drop_first=True, dummy_na=True, dtype=int)\n\n        print(\"Check Nans\")\n        n = X_transformed.isna().mean()\n\n        na_cols = list(n[n > 0].index)\n\n        print(na_cols)\n\n        for col in na_cols:\n            name, pr = col.split(\"_\")\n            if pr != \"woe\":\n                print(\"Error Related to Nans\")\n\n            dic = self.woe_columns_fill_na\n            mappings = self.woe_mappings\n            X_transformed[col] = X_transformed[col].fillna(mappings[name][dic[name]])\n\n            print(col, name, pr, dic[name], mappings[name][dic[name]])\n            \n        return X_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:47.825246Z","iopub.execute_input":"2025-04-10T14:33:47.825636Z","iopub.status.idle":"2025-04-10T14:33:47.839490Z","shell.execute_reply.started":"2025-04-10T14:33:47.825612Z","shell.execute_reply":"2025-04-10T14:33:47.838346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build preprocessor","metadata":{}},{"cell_type":"code","source":"preprocessor = CustomPreprocessor(woe_columns=woe_columns, one_hot_columns=one_hot_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:49.688790Z","iopub.execute_input":"2025-04-10T14:33:49.689086Z","iopub.status.idle":"2025-04-10T14:33:49.694538Z","shell.execute_reply.started":"2025-04-10T14:33:49.689066Z","shell.execute_reply":"2025-04-10T14:33:49.693534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_t = preprocessor.fit_transform(X_train_cleaned, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:50.320376Z","iopub.execute_input":"2025-04-10T14:33:50.321552Z","iopub.status.idle":"2025-04-10T14:33:50.571797Z","shell.execute_reply.started":"2025-04-10T14:33:50.321518Z","shell.execute_reply":"2025-04-10T14:33:50.570711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_t = preprocessor.transform(X_test_cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:52.048058Z","iopub.execute_input":"2025-04-10T14:33:52.048389Z","iopub.status.idle":"2025-04-10T14:33:52.122207Z","shell.execute_reply.started":"2025-04-10T14:33:52.048367Z","shell.execute_reply":"2025-04-10T14:33:52.121042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Selection \n# By Correlation","metadata":{}},{"cell_type":"code","source":"X_corr = X_train_t.copy()\nX_corr['SalePrice'] = y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:53.468223Z","iopub.execute_input":"2025-04-10T14:33:53.468956Z","iopub.status.idle":"2025-04-10T14:33:53.476722Z","shell.execute_reply.started":"2025-04-10T14:33:53.468926Z","shell.execute_reply":"2025-04-10T14:33:53.475685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix = X_corr.corr().abs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:53.859969Z","iopub.execute_input":"2025-04-10T14:33:53.861088Z","iopub.status.idle":"2025-04-10T14:33:53.891405Z","shell.execute_reply.started":"2025-04-10T14:33:53.861037Z","shell.execute_reply":"2025-04-10T14:33:53.890033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\n# Find feature pairs with correlation greater than a threshold\nthreshold = 0.8\nhigh_corr_pairs = []\n\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if corr_matrix.iloc[i, j] > threshold:\n            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\n# Display highly correlated pairs\nif high_corr_pairs:\n    print(\"Highly correlated feature pairs:\")\n    for feat1, feat2, corr in high_corr_pairs:\n        print(f\"{feat1} and {feat2}: {corr:.4f}\")\nelse:\n    print(f\"No feature pairs with correlation above {threshold} found.\")\n\n# To remove one feature from each highly correlated pair\n# (typically the one with lower correlation with target)\nfeatures_to_drop = []\nfor feat1, feat2, _ in high_corr_pairs:\n    # Compare correlation with target\n    if abs(X_train_t[feat1].corr(y)) < abs(X_train_t[feat2].corr(y)):\n        features_to_drop.append(feat1)\n    else:\n        features_to_drop.append(feat2)\n\n# Remove duplicates\nfeatures_to_drop = list(set(features_to_drop))\nprint(f\"Features to drop due to high correlation: {features_to_drop}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:55.153848Z","iopub.execute_input":"2025-04-10T14:33:55.154201Z","iopub.status.idle":"2025-04-10T14:33:55.248493Z","shell.execute_reply.started":"2025-04-10T14:33:55.154181Z","shell.execute_reply":"2025-04-10T14:33:55.246985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_t = X_train_t.drop(columns=features_to_drop)\nX_test_t = X_test_t.drop(columns=features_to_drop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:57.039625Z","iopub.execute_input":"2025-04-10T14:33:57.039943Z","iopub.status.idle":"2025-04-10T14:33:57.049882Z","shell.execute_reply.started":"2025-04-10T14:33:57.039920Z","shell.execute_reply":"2025-04-10T14:33:57.048562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_t.shape,X_test_t.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:57.391944Z","iopub.execute_input":"2025-04-10T14:33:57.392266Z","iopub.status.idle":"2025-04-10T14:33:57.398454Z","shell.execute_reply.started":"2025-04-10T14:33:57.392244Z","shell.execute_reply":"2025-04-10T14:33:57.397490Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RFE","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(\n    scaler.fit_transform(X_train_t),\n    columns=X_train_t.columns,\n    index=X_train_t.index\n)\n\n# Create and fit the model for feature selection only\nmodel = LinearRegression()\nrfe = RFE(estimator=model, n_features_to_select=15, step=1)\nrfe.fit(X_train_scaled, y_train)\n\n# Get selected features\nrfe_selected_features = X_train_t.columns[rfe.support_].tolist()\nprint(\"Features selected by RFE:\")\nfor i, feature in enumerate(rfe_selected_features, 1):\n    print(f\"{i}. {feature}\")\n\n# Create plot for feature ranking\nfeature_ranking = pd.Series(rfe.ranking_, index=X_train_t.columns)\nplt.figure(figsize=(12, 8))\nfeature_ranking.sort_values().head(20).plot(kind='bar')\nplt.title('Top 20 Features by RFE Ranking (lower is better)')\nplt.ylabel('Ranking')\nplt.tight_layout()\nplt.savefig(\"rfe_feature_ranking.png\")\n\n# Create before/after scaling visualization for a few selected features\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(rfe_selected_features[:3], 1):\n    plt.subplot(3, 2, 2*i-1)\n    plt.hist(X_train_t[feature], bins=30)\n    plt.title(f'{feature} - Before Scaling')\n\n    plt.subplot(3, 2, 2*i)\n    plt.hist(X_train_scaled[feature], bins=30)\n    plt.title(f'{feature} - After Scaling')\n\nplt.tight_layout()\nplt.savefig(\"scaling_visualization.png\")\n\n# Create scatter plots of features vs. target for top 5 features\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(rfe_selected_features[:5], 1):  # Plot top 5 features\n    plt.subplot(2, 3, i)\n    plt.scatter(X_train_scaled[feature], y_train, alpha=0.5)\n    plt.title(f'{feature} vs SalePrice')\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n\nplt.tight_layout()\nplt.savefig(\"feature_distributions.png\")\n\n# Show plots if in interactive mode\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:33:59.647997Z","iopub.execute_input":"2025-04-10T14:33:59.648344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build Final Preprocessor","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass FinalCustomPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, woe_columns, one_hot_columns, final_columns):\n        self.woe_columns = woe_columns # Columns Which Should Be Preprocessed Using WOE\n        self.one_hot_columns = one_hot_columns # Columns Which Should Be Preprocessed Using One Hot Encoder\n\n        self.final_columns = final_columns # Columns Selected By RFE\n\n    def fit(self, X, y):\n        # Generate Fill Na Values Just in Case\n        self.woe_columns_fill_na = X[woe_columns].mode().T[0].to_dict()\n        \n        df_woe = X.copy()\n        target_col = 'target'\n        df_woe[target_col] = y\n\n        woe_mappings = {}\n        iv_values = {}\n        \n        for col in self.woe_columns:\n            print(f\"Processing {col}...\")\n            \n            groups = df_woe.groupby([col])[target_col].agg(['count', 'sum'])\n            groups.columns = ['n_obs', 'n_pos']\n            groups['n_neg'] = groups['n_obs'] - groups['n_pos']\n            \n            groups['prop_pos'] = groups['n_pos'] / groups['n_pos'].sum()\n            groups['prop_neg'] = groups['n_neg'] / groups['n_neg'].sum()\n            \n            groups['woe'] = np.log(groups['prop_pos'] / groups['prop_neg'])\n            groups['iv'] = (groups['prop_pos'] - groups['prop_neg']) * groups['woe']\n            \n            groups.replace([np.inf, -np.inf], 0, inplace=True)\n            groups.fillna(0, inplace=True)\n            \n            woe_dict = groups['woe'].to_dict()\n            iv = groups['iv'].sum()\n            \n            woe_mappings[col] = woe_dict\n            iv_values[col] = iv\n\n        self.woe_mappings = woe_mappings\n        self.iv_values = iv_values\n\n        return self\n\n\n    def transform(self, X):\n        X_transformed = X.copy()\n\n        # Preprocess WOE Columns\n        print(\"***\")\n        print(\"Preprocessing WOE Columns\")\n        for col in self.woe_columns:\n            X_transformed[f'{col}_woe'] = X_transformed[col].map(self.woe_mappings[col])\n            X_transformed.drop(columns=col, inplace=True)\n\n        print(\"Preprocessing One Hot Columns\")\n        X_transformed = pd.get_dummies(X_transformed, columns=self.one_hot_columns, drop_first=True, dummy_na=True, dtype=int)\n\n        print(\"Check Nans\")\n        n = X_transformed.isna().mean()\n\n        na_cols = list(n[n > 0].index)\n\n        print(na_cols)\n\n        for col in na_cols:\n            name, pr = col.split(\"_\")\n            if pr != \"woe\":\n                print(\"Error Related to Nans\")\n\n            dic = self.woe_columns_fill_na\n            mappings = self.woe_mappings\n            X_transformed[col] = X_transformed[col].fillna(mappings[name][dic[name]])\n\n            print(col, name, pr, dic[name], mappings[name][dic[name]])\n            \n        return X_transformed[self.final_columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:34:10.666613Z","iopub.execute_input":"2025-04-10T14:34:10.666914Z","iopub.status.idle":"2025-04-10T14:34:10.681508Z","shell.execute_reply.started":"2025-04-10T14:34:10.666892Z","shell.execute_reply":"2025-04-10T14:34:10.680113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_preprocessor = FinalCustomPreprocessor(woe_columns=woe_columns, \n                                             one_hot_columns=one_hot_columns, \n                                             final_columns=rfe_selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:34:12.308235Z","iopub.execute_input":"2025-04-10T14:34:12.308777Z","iopub.status.idle":"2025-04-10T14:34:12.314240Z","shell.execute_reply.started":"2025-04-10T14:34:12.308749Z","shell.execute_reply":"2025-04-10T14:34:12.313235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:34:12.768276Z","iopub.execute_input":"2025-04-10T14:34:12.768627Z","iopub.status.idle":"2025-04-10T14:34:12.792368Z","shell.execute_reply.started":"2025-04-10T14:34:12.768604Z","shell.execute_reply":"2025-04-10T14:34:12.791376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_clean = custom_preprocess(X_train)\nX_test_clean = custom_preprocess(X_test)\ntest_clean = custom_preprocess(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:34:13.520703Z","iopub.execute_input":"2025-04-10T14:34:13.521007Z","iopub.status.idle":"2025-04-10T14:34:13.604542Z","shell.execute_reply.started":"2025-04-10T14:34:13.520985Z","shell.execute_reply":"2025-04-10T14:34:13.603529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_clean.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:34:13.798431Z","iopub.execute_input":"2025-04-10T14:34:13.798754Z","iopub.status.idle":"2025-04-10T14:34:13.804848Z","shell.execute_reply.started":"2025-04-10T14:34:13.798734Z","shell.execute_reply":"2025-04-10T14:34:13.803815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LinearRegression","metadata":{}},{"cell_type":"code","source":"import mlflow\nimport mlflow.sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Define pipeline\npipeline_lr = Pipeline([\n    ('preprocess', FinalCustomPreprocessor(woe_columns=woe_columns, \n                                           one_hot_columns=one_hot_columns, \n                                           final_columns=rfe_selected_features)),\n    ('scaler', StandardScaler()),\n    ('model', LinearRegression())\n])\n\n# Start MLflow experiment/run\nwith mlflow.start_run(run_name=\"LinearRegressionPipeline\"):\n\n    # Fit the model\n    pipeline_lr.fit(X_train_clean, y_train)\n\n    # Predict\n    preds_for_evaluation = pipeline_lr.predict(X_test_clean)\n\n    # Log-transform\n    log_preds = np.log(preds_for_evaluation)\n    log_y_test = np.log(y_test)\n\n    # Metrics\n    rmse = np.sqrt(mean_squared_error(log_y_test, log_preds))\n    bias = np.mean(log_preds - log_y_test)\n    variance = np.var(log_preds)\n\n    # Log parameters (no hyperparams for basic LinearRegression, but we can still log config)\n    mlflow.log_param(\"model\", \"LinearRegression\")\n    mlflow.log_param(\"fit_intercept\", pipeline_lr.named_steps['model'].fit_intercept)\n\n    # Log metrics\n    mlflow.log_metric(\"rmse\", rmse)\n    mlflow.log_metric(\"bias\", bias)\n    mlflow.log_metric(\"variance\", variance)\n\n    # Log the entire pipeline\n    mlflow.sklearn.log_model(pipeline_lr, \"linear_regression_pipeline\")\n\n    print(\"\\n✅ Model and metrics logged to MLflow\")\n    print(f\"→ RMSE: {rmse:.4f}\")\n    print(f\"→ Bias: {bias:.4f}\")\n    print(f\"→ Variance: {variance:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:34:19.068739Z","iopub.execute_input":"2025-04-10T14:34:19.069090Z","iopub.status.idle":"2025-04-10T14:34:28.128188Z","shell.execute_reply.started":"2025-04-10T14:34:19.069066Z","shell.execute_reply":"2025-04-10T14:34:28.127328Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RandomForestRegressor","metadata":{}},{"cell_type":"code","source":"# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.metrics import mean_squared_error\n\n# # Define the pipeline with RandomForestRegressor\n# pipeline_rf = Pipeline([\n#     ('preprocess', FinalCustomPreprocessor(woe_columns=woe_columns, \n#                                            one_hot_columns=one_hot_columns, \n#                                            final_columns=rfe_selected_features)),\n#     ('scaler', StandardScaler()),\n#     ('model', RandomForestRegressor(random_state=42))  # Use RandomForestRegressor\n# ])\n\n# # Fit the model (preprocessing, scaling, and training are done inside the pipeline)\n# pipeline_rf.fit(X_train_clean, y_train)\n\n# # Make predictions on the test set\n# preds_for_evaluation = pipeline_rf.predict(X_test_clean)\n\n# # Log-transform the predictions and actual values\n# log_preds = np.log(preds_for_evaluation)\n# log_y_test = np.log(y_test)  # Assuming y_test contains the actual SalePrice values for the test set\n\n# # Calculate RMSE\n# rmse = np.sqrt(mean_squared_error(log_y_test, log_preds))\n# print(f\"RMSE: {rmse}\")\n\n# # Calculate Bias\n# bias = np.mean(log_preds - log_y_test)\n# print(f\"Bias: {bias}\")\n\n# # Calculate Variance\n# variance = np.var(log_preds)\n# print(f\"Variance: {variance}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T13:23:50.720342Z","iopub.execute_input":"2025-04-10T13:23:50.720675Z","iopub.status.idle":"2025-04-10T13:23:51.856110Z","shell.execute_reply.started":"2025-04-10T13:23:50.720651Z","shell.execute_reply":"2025-04-10T13:23:51.855209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBRegressor","metadata":{}},{"cell_type":"code","source":"# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler\n# from xgboost import XGBRegressor\n# from sklearn.metrics import mean_squared_error\n# import numpy as np\n\n# # Define the pipeline with XGBRegressor\n# pipeline_xgb = Pipeline([\n#     ('preprocess', FinalCustomPreprocessor(woe_columns=woe_columns, \n#                                            one_hot_columns=one_hot_columns, \n#                                            final_columns=rfe_selected_features)),\n#     ('scaler', StandardScaler()),\n#     ('model', XGBRegressor(random_state=42))  # Use XGBRegressor\n# ])\n\n# # Fit the model (preprocessing, scaling, and training are done inside the pipeline)\n# pipeline_xgb.fit(X_train_clean, y_train)\n\n# # Make predictions on the test set\n# preds_for_evaluation = pipeline_xgb.predict(X_test_clean)\n\n# # Log-transform the predictions and actual values\n# log_preds = np.log(preds_for_evaluation)\n# log_y_test = np.log(y_test)  # Assuming y_test contains the actual SalePrice values for the test set\n\n# # Calculate RMSE\n# rmse = np.sqrt(mean_squared_error(log_y_test, log_preds))\n# print(f\"RMSE: {rmse}\")\n\n# # Calculate Bias\n# bias = np.mean(log_preds - log_y_test)\n# print(f\"Bias: {bias}\")\n\n# # Calculate Variance\n# variance = np.var(log_preds)\n# print(f\"Variance: {variance}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T13:23:54.239788Z","iopub.execute_input":"2025-04-10T13:23:54.240096Z","iopub.status.idle":"2025-04-10T13:23:54.739179Z","shell.execute_reply.started":"2025-04-10T13:23:54.240074Z","shell.execute_reply":"2025-04-10T13:23:54.736914Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GradientBoostingRegressor","metadata":{}},{"cell_type":"code","source":"# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.ensemble import GradientBoostingRegressor\n# from sklearn.metrics import mean_squared_error\n# import numpy as np\n\n# # Define the pipeline with GradientBoostingRegressor\n# pipeline_gb = Pipeline([\n#     ('preprocess', FinalCustomPreprocessor(woe_columns=woe_columns, \n#                                            one_hot_columns=one_hot_columns, \n#                                            final_columns=rfe_selected_features)),\n#     ('scaler', StandardScaler()),\n#     ('model', GradientBoostingRegressor(\n#     n_estimators=100,\n#     learning_rate=0.1,\n#     max_depth=3,\n#     subsample=1.0,\n#     random_state=None\n# ))  # Use GradientBoostingRegressor\n# ])\n\n# # Fit the model (preprocessing, scaling, and training are done inside the pipeline)\n# pipeline_gb.fit(X_train_clean, y_train)\n\n# # Make predictions on the test set\n# preds_for_evaluation = pipeline_gb.predict(X_test_clean)\n\n# # Log-transform the predictions and actual values\n# log_preds = np.log(preds_for_evaluation)\n# log_y_test = np.log(y_test)  # Assuming y_test contains the actual SalePrice values for the test set\n\n# # Calculate RMSE\n# rmse = np.sqrt(mean_squared_error(log_y_test, log_preds))\n# print(f\"RMSE: {rmse}\")\n\n# # Calculate Bias\n# bias = np.mean(log_preds - log_y_test)\n# print(f\"Bias: {bias}\")\n\n# # Calculate Variance\n# variance = np.var(log_preds)\n# print(f\"Variance: {variance}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T13:24:00.848169Z","iopub.execute_input":"2025-04-10T13:24:00.848466Z","iopub.status.idle":"2025-04-10T13:24:01.468725Z","shell.execute_reply.started":"2025-04-10T13:24:00.848446Z","shell.execute_reply":"2025-04-10T13:24:01.467575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='AleksandreBakhtadze', repo_name='ML-abakh22-assignment-1', mlflow=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T13:55:41.195850Z","iopub.execute_input":"2025-04-10T13:55:41.196201Z","iopub.status.idle":"2025-04-10T13:56:06.493192Z","shell.execute_reply.started":"2025-04-10T13:55:41.196172Z","shell.execute_reply":"2025-04-10T13:56:06.492136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mlflow\nwith mlflow.start_run():\n  mlflow.log_param('parameter name', 'value')\n  mlflow.log_metric('metric name', 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:03:23.803260Z","iopub.execute_input":"2025-04-10T14:03:23.803651Z","iopub.status.idle":"2025-04-10T14:03:29.024140Z","shell.execute_reply.started":"2025-04-10T14:03:23.803628Z","shell.execute_reply":"2025-04-10T14:03:29.023076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}